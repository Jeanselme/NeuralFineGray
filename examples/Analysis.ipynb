{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file allows to analyze results obtained by running experiments_competing_risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "sys.path.append('../DeepSurvivalMachines/')\n",
    "from nfg import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to analyze other datasets result\n",
    "dataset = 'SEER' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../Results/' # Path where the data is saved\n",
    "x, t, e, cNCriates = datasets.load_dataset(dataset, competing = True, normalize = False) # Open the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sksurv.metrics import concordance_index_ipcw, brier_score, cumulative_dynamic_auc, integrated_brier_score\n",
    "\n",
    "### Utils: The evaluatino metrics used\n",
    "def evaluate(survival, e = e, t = t, groups = None):\n",
    "    folds = survival.iloc[:, -1].values\n",
    "    survival = survival.iloc[:, :-1]\n",
    "    times = survival.columns.get_level_values(1).unique()\n",
    "    risk = 1 - survival\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # If multiple risk, compute cause specific metrics\n",
    "    for r in survival.columns.get_level_values(0).unique():\n",
    "        for fold in np.arange(5):\n",
    "            e_train, t_train = e[folds != fold], t[folds != fold]\n",
    "            e_test,  t_test  = e[folds == fold], t[folds == fold]\n",
    "            g_train, g_test = (None, None) if groups is None else (groups[folds != fold], groups[folds == fold])\n",
    "\n",
    "            et_train = np.array([(e_train[i] == int(r), t_train[i]) for i in range(len(e_train))], # For estimation censoring\n",
    "                            dtype = [('e', bool), ('t', float)])\n",
    "            et_test = np.array([(e_test[i] == int(r), t_test[i]) for i in range(len(e_test))], # For measure performance for given outcome\n",
    "                            dtype = [('e', bool), ('t', float)])\n",
    "            selection = (t_test < t_train.max()) | (e[folds == fold] != int(r))\n",
    "            \n",
    "            et_test, g_test = et_test[selection], None if groups is None else g_test[selection]\n",
    "            survival_fold = survival[folds == fold][r][selection]\n",
    "            risk_fold = risk[folds == fold][r][selection]\n",
    "\n",
    "            try:\n",
    "                brs = brier_score(et_train, et_test, survival_fold.values, times)[1]\n",
    "            except:\n",
    "                brs = [np.nan] * len(times)\n",
    "            # Concordance and ROC for each time\n",
    "            gcis, cis, rocs = [], [], []\n",
    "            res_group = {} \n",
    "            if groups is not None:\n",
    "                res_group = {\"CIS_{}\".format(group): [] for group in groups.unique()}\n",
    "                res_group.update({\"BRS_{}\".format(group): brier_score(et_train[g_train == group], et_test[g_test == group], survival_fold[g_test == group], times)[1] for group in groups.unique()})\n",
    "\n",
    "            for time in times:\n",
    "                try:\n",
    "                    gcis.append(concordance_index_ipcw(et_train, et_test, risk_fold[time])[0])\n",
    "                except:\n",
    "                    gcis.append(np.nan)\n",
    "                    \n",
    "                try:\n",
    "                    cis.append(concordance_index_ipcw(et_train, et_test, risk_fold[time], float(time))[0])\n",
    "                except:\n",
    "                    cis.append(np.nan)\n",
    "\n",
    "                try:\n",
    "                    rocs.append(cumulative_dynamic_auc(et_train, et_test, risk_fold[time], float(time))[0][0])\n",
    "                except:\n",
    "                    rocs.append(np.nan)\n",
    "\n",
    "                try:\n",
    "                    for group in groups.unique():\n",
    "                        res_group[\"CIS_{}\".format(group)].append(concordance_index_ipcw(et_train[g_train == group], et_test[g_test == group], risk_fold[time][g_test == group], float(time))[0])\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            res = {\"GCIS\": gcis, \"CIS\": cis, \"BRS\": brs, \"ROCS\": rocs}\n",
    "            if groups is not None:\n",
    "                res.update(res_group)\n",
    "            results[(r, fold)] = pd.DataFrame.from_dict(res, orient='index', columns = times)\n",
    "    results = pd.concat(results)\n",
    "    results.index.set_names(['Risk', 'Fold', 'Metric'], inplace = True)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To analyze group performance - We did this only for FRAMINGHAM\n",
    "if dataset == \"FRAMINGHAM\":\n",
    "    groups = pd.DataFrame(x, columns = cNCriates).AGE\n",
    "    groups = pd.cut(groups, [0, 40, 50, 60, 100], labels=[\"<40\", '40-50', \"50-60\", \"60+\"])\n",
    "    print(groups.value_counts())\n",
    "    for g in groups.unique().sort_values():\n",
    "        print(\"Group {} - Population {} - Outcome {:.2f}% - Censoring {:.2f}%\".format(g, (groups == g).sum(), 100 * (e[groups == g] == 2).mean(),\n",
    "                                                                                        100 * (e[groups == g] == 0).mean()))\n",
    "else:\n",
    "    groups = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment import Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open file and compute performance\n",
    "predictions, clusters, results, likelihood, models = {}, {}, {}, {}, {}\n",
    "for file_name in os.listdir(path):\n",
    "    if dataset in file_name and '.csv' in file_name: \n",
    "        model = file_name       \n",
    "        model = model[model.rindex('_') + 1: model.index('.')]\n",
    "\n",
    "        print(\"Opening :\", file_name, ' - ', model)\n",
    "        if 'ds' != model:\n",
    "            continue\n",
    "\n",
    "        if 'finegray' in model or 'cox' in model:\n",
    "            # Reinitialize index\n",
    "            predictions[model] = pd.read_csv(path + file_name, header = [0], index_col = 0)\n",
    "            index = pd.DataFrame([[i, t] for i in ('1', '2') for t in predictions[model].columns[:3]] + [['Use', '']])\n",
    "            predictions[model].columns = pd.MultiIndex.from_frame(index)\n",
    "        else:\n",
    "            predictions[model] = pd.read_csv(path + file_name, header = [0, 1], index_col = 0)\n",
    "        results[model] = evaluate(predictions[model], groups = groups)\n",
    "\n",
    "        model_file = file_name[: file_name.index('.')] + '.pickle'\n",
    "        try:\n",
    "            models[model] = Experiment.load(path + model_file)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        cluster_file = file_name[: file_name.index('.')] + '_clusters.pickle'\n",
    "        if os.path.isfile(path + cluster_file):\n",
    "            clusters[model] = pickle.load(open(path + cluster_file, 'rb'))\n",
    "        \n",
    "\n",
    "# Rename\n",
    "# TODO: Add your method in the list for nicer display\n",
    "dict_name = {'nfg': 'NeuralFG', 'nfgcs': 'NeuralFG NC', 'finegray': 'Fine Gray', 'dsm': 'DSM', 'dsmcs': 'DSM NC', 'dh': 'DeepHit', 'dhcs': 'DeepHit NC', 'ds': 'DeSurv', 'dscs': 'DeSurv NC'} \n",
    "\n",
    "likelihood = pd.DataFrame.from_dict(likelihood, 'index').rename(dict_name)\n",
    "results = pd.concat(results).rename(dict_name)\n",
    "results.index.set_names('Model', 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = results.groupby(['Model', 'Risk', 'Metric']).apply(lambda x: pd.Series([\"{:.3f} ({:.2f})\".format(mean, std) for mean, std in zip(x.mean(), x.std())], index = x.columns.astype(float)))\n",
    "table = table.unstack(level=-1).stack(level=0).unstack(level=-1).loc[:, ['CIS', 'BRS']] \n",
    "table = table.reorder_levels(['Risk', 'Model']).sort_index(level = 0, sort_remaining = False)\n",
    "\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(table.style.to_latex())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split by age"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is to be used for the FRAMINGHAM analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = results.groupby(['Model', 'Risk', 'Metric']).apply(lambda x: pd.Series([\"{:.3f} ({:.2f})\".format(mean, std) for mean, std in zip(x.mean(), x.std())], index = x.columns.astype(float)))\n",
    "table = table.loc[table.index.get_level_values(2).str.contains('BRS_')].unstack(level=-1).stack(level=0).loc[['NeuralFG', 'NeuralFG NC'], ['BRS_<40', 'BRS_40-50', 'BRS_50-60', 'BRS_60+']]\n",
    "table = table.reorder_levels(['Risk', 'Model', None]).sort_index(level = 0, sort_remaining = False)\n",
    "\n",
    "difference = (results.loc['NeuralFG'] - results.loc['NeuralFG NC']).groupby(['Risk', 'Metric']).apply(lambda x: pd.Series([\"{:.3f} ({:.2f})\".format(mean, std) for mean, std in zip(x.mean(), x.std())], index = x.columns.astype(float)))\n",
    "difference = difference.loc[difference.index.get_level_values(1).str.contains('BRS_')].unstack(level=-1).stack(level=0).loc[:, ['BRS_<40', 'BRS_40-50', 'BRS_50-60', 'BRS_60+']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = table.loc['2'].T.stack().reorder_levels([None, 'Metric']).sort_index(level = 0, sort_remaining = False)\n",
    "table['Difference'] = difference.loc['2'].stack()\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(table.unstack().T.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.concat({\"Age Group\": groups, \"Event\": pd.Series(e)}, 1).groupby(['Age Group', 'Event']).size().unstack().to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat({\"Age Group\": groups, \"Event\": pd.Series(e)}, 1).groupby(['Age Group', 'Event']).size().unstack().rename(columns = {0: 'Censoring', 1: 'Death', 2: 'CVD'})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance\n",
    "\n",
    "Estimate the feature importance of models with and without competing risks to understand how important is to leverage this information. You need to have run the cause specific neural fine gray model (option cause_specific)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_interest = 1\n",
    "iter = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, t, e, cNCriates = datasets.load_dataset(dataset, competing = True, normalize = True) # Open the data\n",
    "t =  models['nfg'].__preprocess__(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cNCriates = pd.Series(cNCriates).replace({\n",
    "    'SEX': 'Sex',\n",
    "    'CURSMOKE': 'Smoking',\n",
    "    'DIABETES': 'Diabetes',\n",
    "    'BPMEDS': 'Anti-hypertensive medication',\n",
    "    'educ': 'Education',\n",
    "    'PREVCHD': 'Coronary Heart Disease',\n",
    "    'PREVAP': 'Angina Pectoris',\n",
    "    'PREVMI': ' Myocardial Infraction',\n",
    "    'PREVSTRK': 'Stroke',\n",
    "    'PREVHYP': 'Hypotension',\n",
    "    'TOTCHOL': 'Cholesterol',\n",
    "    'AGE': 'Age',\n",
    "    'SYSBP': 'Systolic Blood Pressure',\n",
    "    'DIABP': 'Diastolic Blood Pressure',\n",
    "    'CIGPDAY': 'Number of cigarettes',\n",
    "    'HEARTRTE': 'Heart rate',\n",
    "    'GLUCOSE' : 'Glucose'\n",
    "}).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = {'Competing': [], 'Non Competing': []}\n",
    "\n",
    "for fold in range(5):\n",
    "    # Competing risk importance\n",
    "    competing_mean, competing_std = models['nfg'].best_model[fold].feature_importance(x, t, e, iter)\n",
    "    importance['Competing'].append((pd.Series(competing_mean), pd.Series(competing_std)))\n",
    "\n",
    "    ncompeting_mean, ncompeting_std = models['nfgcs'].best_model[fold].feature_importance(x, t, e, iter)\n",
    "    importance['Non Competing'].append((pd.Series(ncompeting_mean), pd.Series(ncompeting_std)))\n",
    "for model in importance:\n",
    "    mean, std = pd.concat([impi[0] for impi in importance[model]], axis = 1), pd.concat([impi[1] for impi in importance[model]], axis = 1)\n",
    "    importance[model] = pd.concat({\"Error\": std.mean(1), \"Mean\": mean.mean(1)}, axis = 1) # Wrong error as correlation may imapct\n",
    "\n",
    "importance = pd.concat(importance, axis = 1)\n",
    "importance.index = cNCriates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort = importance[('Competing', 'Mean')].abs().sort_values().index\n",
    "importance = importance.loc[sort]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(\"whitegrid\"):\n",
    "    importance.iloc[-6:,[1, 3]].droplevel(1, axis = 1).plot.barh(xerr = (importance.iloc[-6:,0], importance.iloc[-6:,2]))\n",
    "    plt.xlim(0, 0.8)\n",
    "    plt.xlabel('Relative change in NLL')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Risk sets - Guidelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, t, e, cNCriates = datasets.load_dataset(dataset, competing = True, normalize = True) # Open the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten_year = models['nfg'].__preprocess__(3650) # To adapt if the dataset is not in dayss\n",
    "labels = [\"Low\", \"Medium\", \"High\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten_year_survival = {'nfg': pd.Series(0, index = predictions['nfg'].index), 'nfgcs': pd.Series(0, index = predictions['nfg'].index)}\n",
    "for fold in range(5):\n",
    "    index = (predictions['nfg'].Use == fold).iloc[:, 0]\n",
    "\n",
    "    # Competing risk importance\n",
    "    for model in ten_year_survival:\n",
    "        risks = models[model].best_model[fold].predict_risk(x[index], [ten_year], risk = 2).flatten() # Predict CVD risk\n",
    "        ten_year_survival[model][index] = pd.cut(risks, [0, 0.1, 0.2, 1], labels = labels).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = (groups == '50-60') | (groups == '60+')\n",
    "analysisGroup = {\n",
    "    'All': selection,\n",
    "    'Observed event': (t <= 3650) & (e == 2) & selection,\n",
    "    'No event': (t > 3650) & selection\n",
    "}\n",
    "\n",
    "for group in analysisGroup:\n",
    "    confusion = pd.DataFrame(confusion_matrix(ten_year_survival['nfg'][analysisGroup[group]], ten_year_survival['nfgcs'][analysisGroup[group]], labels = labels), index = labels, columns = labels) # line represents nfg, columns single nfg\n",
    "    confusion = pd.concat([confusion, confusion.sum(1).rename('Competing')], 1)\n",
    "    confusion.loc['Non Competing'] = confusion.sum(0)\n",
    "    print(group, confusion.to_latex())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "survival",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f1b50223f39b64c0c24545f474e3e7d2d3b4b121fe045100fc03a3926bb649af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file allows to analyze results obtained by running experiments_competing_risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "sys.path.append('../DeepSurvivalMachines/')\n",
    "from nfg import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to analyze other datasets result\n",
    "dataset = 'SYNTHETIC_COMPETING' \n",
    "group_selection = 'sex' # For FRAMINGHAM, a more detailed analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../Results/' # Path where the data is saved\n",
    "x, t, e, cNCriates = datasets.load_dataset(dataset, path = '../', competing = True, normalize = False) # Open the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizons = [0.25, 0.5, 0.75] # Horizons to evaluate the models\n",
    "times_eval = np.quantile(t[e > 0], horizons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycox.evaluation import EvalSurv\n",
    "from sksurv.metrics import concordance_index_ipcw, brier_score, cumulative_dynamic_auc, integrated_brier_score\n",
    "\n",
    "### Utils: The evaluatino metrics used\n",
    "def evaluate(survival, e = e, t = t, groups = None, times_eval = []):\n",
    "    folds = survival.iloc[:, -1].values\n",
    "    survival = survival.iloc[:, :-1]\n",
    "    survival.columns = pd.MultiIndex.from_frame(pd.DataFrame(index=survival.columns).reset_index().astype(float))\n",
    "    \n",
    "    times = survival.columns.get_level_values(1).unique()\n",
    "    results = {}\n",
    "\n",
    "    # If multiple risk, compute cause specific metrics\n",
    "    for r in survival.columns.get_level_values(0).unique():\n",
    "        for fold in np.arange(5):\n",
    "            res = {}\n",
    "            e_train, t_train = e[folds != fold], t[folds != fold]\n",
    "            e_test,  t_test  = e[folds == fold], t[folds == fold]\n",
    "            g_train, g_test = (None, None) if groups is None else (groups[folds != fold], groups[folds == fold])\n",
    "\n",
    "            et_train = np.array([(e_train[i] == int(r), t_train[i]) for i in range(len(e_train))], # For estimation censoring\n",
    "                            dtype = [('e', bool), ('t', float)])\n",
    "            et_test = np.array([(e_test[i] == int(r), t_test[i]) for i in range(len(e_test))], # For measure performance for given outcome\n",
    "                            dtype = [('e', bool), ('t', float)])\n",
    "            \n",
    "            selection = (t_test < t_train.max()) | (e[folds == fold] != int(r))\n",
    "            \n",
    "            et_test, g_test = et_test[selection], None if groups is None else g_test[selection]\n",
    "            survival_train = survival[folds != fold][r]\n",
    "            survival_fold = survival[folds == fold][r]\n",
    "\n",
    "            km = EvalSurv(survival_train.T, t_train, e_train == int(r), censor_surv = 'km')\n",
    "            test_eval = EvalSurv(survival_fold.T, t_test, e_test == int(r), censor_surv = km)\n",
    "\n",
    "            res['Overall'] = {\n",
    "                    \"CIS\": test_eval.concordance_td(), \n",
    "                }\n",
    "            try:\n",
    "                res['Overall']['BRS'] = test_eval.integrated_brier_score(times.to_numpy())\n",
    "            except: pass\n",
    "\n",
    "            \n",
    "            if len(times_eval) > 0:\n",
    "                indexes = [np.argmin(np.abs(times - te)) for te in times_eval]\n",
    "                briers = brier_score(et_train, et_test, survival_fold[selection].iloc[:, indexes], times_eval)[1]\n",
    "                for te, brier, index in zip(times_eval, briers, indexes):\n",
    "                    try:\n",
    "                        res[te] = {\n",
    "                            \"CIS\": concordance_index_ipcw(et_train, et_test, 1 - survival_fold[selection].iloc[:, index], te)[0], \n",
    "                            \"BRS\": brier,\n",
    "                            \"ROCS\": cumulative_dynamic_auc(et_train, et_test, 1 - survival_fold[selection].iloc[:, index], te)[0][0]}\n",
    "                    except:\n",
    "                        pass\n",
    "                \n",
    "                    for group in groups.unique() if groups is not None else []:\n",
    "                        try:\n",
    "                            res[te][\"CIS_{}\".format(group)] = concordance_index_ipcw(et_train[g_train == group], et_test[g_test == group], 1 - survival_fold[selection][g_test == group].iloc[:, index], te)[0]\n",
    "                            res[te][\"BRS_{}\".format(group)] = brier_score(et_train[g_train == group], et_test[g_test == group], survival_fold[selection][g_test == group].iloc[:, index], [te])[1][0]\n",
    "\n",
    "                            res[te][\"Delta_CIS_{}\".format(group)] = res[te][\"CIS_{}\".format(group)] - concordance_index_ipcw(et_train[g_train != group], et_test[g_test != group], 1 - survival_fold[selection][g_test != group].iloc[:, index], te)[0]\n",
    "                            res[te][\"Delta_BRS_{}\".format(group)] = res[te][\"BRS_{}\".format(group)] - brier_score(et_train[g_train != group], et_test[g_test != group], survival_fold[selection][g_test != group].iloc[:, index], [te])[1][0]\n",
    "                        \n",
    "                        except:\n",
    "                            pass\n",
    "            results[(r, fold)] = pd.DataFrame.from_dict(res)\n",
    "    results = pd.concat(results)\n",
    "    results.index.set_names(['Risk', 'Fold', 'Metric'], inplace = True)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To analyze group performance - We did this only for FRAMINGHAM\n",
    "if dataset == \"FRAMINGHAM\":\n",
    "    if group_selection == 'sex':\n",
    "        groups = pd.DataFrame(x, columns = cNCriates).SEX.replace({1: 'Male', 2: 'Female'})\n",
    "        print(groups.value_counts())\n",
    "        for g in groups.unique():\n",
    "            print(\"Group {} - Population {} - Outcome {:.2f}% - Censoring {:.2f}%\".format(g, (groups == g).sum(), 100 * (e[groups == g] == 1).mean(),\n",
    "                                                                                            100 * (e[groups == g] == 0).mean()))\n",
    "    else:\n",
    "        groups = pd.DataFrame(x, columns = cNCriates).AGE\n",
    "        groups = pd.cut(groups, [0, 40, 50, 60, 100], labels=[\"<40\", '40-50', \"50-60\", \"60+\"])\n",
    "        print(groups.value_counts())\n",
    "        for g in groups.unique().sort_values():\n",
    "            print(\"Group {} - Population {} - Outcome {:.2f}% - Censoring {:.2f}%\".format(g, (groups == g).sum(), 100 * (e[groups == g] == 2).mean(),\n",
    "                                                                                        100 * (e[groups == g] == 0).mean()))\n",
    "else:\n",
    "    groups = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment import Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open file and compute performance\n",
    "predictions, clusters, results, likelihood, models = {}, {}, {}, {}, {}\n",
    "for file_name in os.listdir(path):\n",
    "    if dataset in file_name and '.csv' in file_name: \n",
    "        model = file_name       \n",
    "        model = model[model.rindex('_') + 1: model.index('.')]\n",
    "        print(\"Opening :\", file_name, ' - ', model)\n",
    "        \n",
    "        if 'finegray' in model or 'cox' in model:\n",
    "            # Reinitialize index\n",
    "            predictions[model] = pd.read_csv(path + file_name, header = [0], index_col = 0).T.ffill().T\n",
    "            index = pd.DataFrame([[i, t] for i in ('1', '2') for t in predictions[model].columns[:100]] + [['Use', '']])\n",
    "            predictions[model].columns = pd.MultiIndex.from_frame(index)\n",
    "        else:\n",
    "            predictions[model] = pd.read_csv(path + file_name, header = [0, 1], index_col = 0)\n",
    "\n",
    "        results[model] = evaluate(predictions[model], groups = groups, times_eval = times_eval)\n",
    "\n",
    "        model_file = file_name[: file_name.index('.')] + '.pickle'\n",
    "        try:\n",
    "            models[model] = Experiment.load(path + model_file)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        cluster_file = file_name[: file_name.index('.')] + '_clusters.pickle'\n",
    "        if os.path.isfile(path + cluster_file):\n",
    "            clusters[model] = pickle.load(open(path + cluster_file, 'rb'))\n",
    "\n",
    "# Rename\n",
    "# TODO: Add your method in the list for nicer display\n",
    "dict_name = {'nfg': 'NeuralFG', 'nfgcs': 'NeuralFG Non Competing', 'finegray': 'Fine Gray', 'dsm': 'DSM', 'dh': 'DeepHit', 'ds': 'DeSurv', 'coxcs': 'CS Cox'} \n",
    "\n",
    "likelihood = pd.DataFrame.from_dict(likelihood, 'index').rename(dict_name)\n",
    "results = pd.concat(results).rename(dict_name)\n",
    "results.index.set_names('Model', 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average performance across fold and models\n",
    "table = results.groupby(['Model', 'Risk', 'Metric']).apply(lambda x:  pd.Series([\"{:.3f} ({:.3f})\".format(mean, std) for mean, std in zip(x.mean(), x.std())], index = x.columns))\n",
    "table = table.unstack(level=-1).stack(level=0).unstack(level=-1).loc[:, ['CIS']]\n",
    "table = table.reorder_levels(['Risk', 'Model']).sort_index(level = 0, sort_remaining = False)\n",
    "\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(table.droplevel(0, axis = 1).to_latex())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split by age"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is to be used for the FRAMINGHAM analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert dataset == \"FRAMINGHAM\", \"All following analysis is specific to FRAMINGHAM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = ['BRS_Male', 'BRS_Female'] if group_selection == 'sex' else ['BRS_<40', 'BRS_40-50', 'BRS_50-60', 'BRS_60+']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = results.groupby(['Model', 'Risk', 'Metric']).apply(lambda x: pd.Series([\"{:.3f} ({:.3f})\".format(mean, std) for mean, std in zip(x.mean(), x.std())], index = x.columns))\n",
    "table = table.unstack(level=-1).stack(level=0).loc[['NeuralFG', 'NeuralFG Non Competing'], selection]\n",
    "table = table.reorder_levels(['Risk', 'Model', None]).sort_index(level = 0, sort_remaining = False)\n",
    "\n",
    "difference = (results.loc['NeuralFG'] - results.loc['NeuralFG Non Competing']).groupby(['Risk', 'Metric']).apply(lambda x: pd.Series([\"{:.3f} ({:.3f})\".format(mean, std) for mean, std in zip(x.mean(), x.std())], index = x.columns))\n",
    "difference = difference.unstack(level=-1).stack(level=0).loc[:, selection]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = table.loc[2].T.stack().reorder_levels([None, 'Metric']).sort_index(level = 0, sort_remaining = False)\n",
    "table['Difference'] = difference.loc[2].stack()\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(table.unstack().T.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.concat({\"Age Group\": groups, \"Event\": pd.Series(e)}, 1).groupby(['Age Group', 'Event']).size().unstack().to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat({\"Age Group\": groups, \"Event\": pd.Series(e)}, 1).groupby(['Age Group', 'Event']).size().unstack().rename(columns = {0: 'Censoring', 1: 'Death', 2: 'CVD'})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance\n",
    "\n",
    "Estimate the feature importance of models with and without competing risks to understand how important is to leverage this information. You need to have run the cause specific neural fine gray model (option cause_specific)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_interest = 2 \n",
    "iter = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, t, e, cNCriates = datasets.load_dataset(dataset, competing = True, normalize = True) # Open the data\n",
    "t =  models['nfg'].__preprocess__(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cNCriates = pd.Series(cNCriates).replace({\n",
    "    'SEX': 'Sex',\n",
    "    'CURSMOKE': 'Smoking',\n",
    "    'DIABETES': 'Diabetes',\n",
    "    'BPMEDS': 'Anti-hypertensive medication',\n",
    "    'educ': 'Education',\n",
    "    'PREVCHD': 'Coronary Heart Disease',\n",
    "    'PREVAP': 'Angina Pectoris',\n",
    "    'PREVMI': ' Myocardial Infraction',\n",
    "    'PREVSTRK': 'Stroke',\n",
    "    'PREVHYP': 'Hypotension',\n",
    "    'TOTCHOL': 'Cholesterol',\n",
    "    'AGE': 'Age',\n",
    "    'SYSBP': 'Systolic Blood Pressure',\n",
    "    'DIABP': 'Diastolic Blood Pressure',\n",
    "    'CIGPDAY': 'Number of cigarettes',\n",
    "    'HEARTRTE': 'Heart rate',\n",
    "    'GLUCOSE' : 'Glucose'\n",
    "}).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = {'Competing': [], 'Non Competing': []}\n",
    "\n",
    "for fold in range(5):\n",
    "    # Competing risk importance\n",
    "    competing_mean, competing_std = models['nfg'].best_model[fold].feature_importance(x, t, e, iter)\n",
    "    importance['Competing'].append((pd.Series(competing_mean), pd.Series(competing_std)))\n",
    "\n",
    "    ncompeting_mean, ncompeting_std = models['nfgcs'].best_model[fold].feature_importance(x, t, e, iter)\n",
    "    importance['Non Competing'].append((pd.Series(ncompeting_mean), pd.Series(ncompeting_std)))\n",
    "for model in importance:\n",
    "    mean, std = pd.concat([impi[0] for impi in importance[model]], axis = 1), pd.concat([impi[1] for impi in importance[model]], axis = 1)\n",
    "    importance[model] = pd.concat({\"Error\": std.mean(1), \"Mean\": mean.mean(1)}, axis = 1) # Wrong error as correlation may imapct\n",
    "\n",
    "importance = pd.concat(importance, axis = 1)\n",
    "importance.index = cNCriates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort = importance[('Competing', 'Mean')].abs().sort_values().index\n",
    "importance = importance.loc[sort]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(\"whitegrid\"):\n",
    "    importance.iloc[-6:,[1, 3]].droplevel(1, axis = 1).plot.barh(xerr = (importance.iloc[-6:,0], importance.iloc[-6:,2]))\n",
    "    plt.xlim(0, 0.8)\n",
    "    plt.xlabel('Relative change in NLL')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Risk sets - Guidelines"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use with the Framingham dataset to explore how this impact the 10-year risk estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, t, e, cNCriates = datasets.load_dataset(dataset, competing = True, normalize = True) # Open the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten_year = models['nfg'].__preprocess__(3650) # To adapt if the dataset is not in dayss\n",
    "labels = [\"Low\", \"Medium\", \"High\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten_year_survival = {'nfg': pd.Series('', index = predictions['nfg'].index), 'nfgcs': pd.Series('', index = predictions['nfg'].index)}\n",
    "for fold in range(5):\n",
    "    index = (predictions['nfg'].Use == fold).iloc[:, 0]\n",
    "\n",
    "    # Competing risk importance\n",
    "    for model in ten_year_survival:\n",
    "        risks = models[model].best_model[fold].predict_risk(x[index], [ten_year], risk = 2).flatten() # Predict CVD risk\n",
    "        ten_year_survival[model][index] = pd.cut(risks, [0, 0.1, 0.2, 1], labels = labels).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = (groups == '50-60') | (groups == '60+')\n",
    "analysisGroup = {\n",
    "    'All': selection,\n",
    "    'Observed event': (t <= 3650) & (e == 2) & selection,\n",
    "    'No event': (t > 3650) & selection\n",
    "}\n",
    "\n",
    "for group in analysisGroup:\n",
    "    confusion = pd.DataFrame(confusion_matrix(ten_year_survival['nfg'][analysisGroup[group]], ten_year_survival['nfgcs'][analysisGroup[group]], labels = labels), index = labels, columns = labels) # line represents nfg, columns single nfg\n",
    "    confusion = pd.concat([confusion, confusion.sum(1).rename('Competing')], 1)\n",
    "    confusion.loc['Non Competing'] = confusion.sum(0)\n",
    "    print(group, confusion.to_latex())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Displaying survival curves\n",
    "\n",
    "Displays the risk scores estimated by both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_display = ['nfg', 'dh', 'ds', 'dsm', 'finegray', 'coxcs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "patient_id = np.random.choice(len(predictions['nfg']), size = 5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(len(patient_id), 2, sharex=True, sharey='row', figsize = (8, len(patient_id) * 2))\n",
    "fig.supxlabel('Time (in years)')\n",
    "fig.supylabel('Cumulative Incidence Functions')\n",
    "plt.xlim(0, 25)\n",
    "\n",
    "for i, id in enumerate(patient_id):\n",
    "    for j, risk in enumerate(['1', '2']):\n",
    "        for model in models_display:\n",
    "            pred_pat = predictions[model][risk].loc[id].rename(dict_name[model])\n",
    "            pred_pat.index = pred_pat.index.astype(float) / 365\n",
    "\n",
    "            pred_pat.plot(legend = False, ax = axes[i, j])\n",
    "    \n",
    "        axes[i, j].grid(alpha = 0.3)\n",
    "    axes[i, 0].set_ylabel(f'Patient {i + 1}')\n",
    "\n",
    "axes[0, 0].set_title('Death')\n",
    "axes[0, 1].set_title('CVD')\n",
    "axes[0, 1].legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "survival",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f1b50223f39b64c0c24545f474e3e7d2d3b4b121fe045100fc03a3926bb649af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

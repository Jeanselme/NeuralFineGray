{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speed comparison"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will apply Neural Fine Gray and Desurv on the FRAMINGHAM data and measure the execution times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../DeepSurvivalMachines/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the FRAMINGHAM Dataset\n",
    "\n",
    "The package includes helper functions to load the dataset.\n",
    "\n",
    "X represents an np.array of features (covariates),\n",
    "T is the event/censoring times and,\n",
    "E is the censoring indicator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nfg import datasets\n",
    "x, t, e, columns = datasets.load_dataset('FRAMINGHAM', competing = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute horizons at which we evaluate the performance of Neural Fine Gray\n",
    "\n",
    "Survival predictions are issued at certain time horizons. Here we will evaluate the performance\n",
    "of NFG to issue predictions at the 25th, 50th and 75th event time quantile as is standard practice in Survival Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "np.random.seed(42)\n",
    "torch.random.manual_seed(42)\n",
    "\n",
    "horizons = [0.25, 0.5, 0.75]\n",
    "times = np.quantile(t[e > 0], horizons) # Fixed horizons for accurate comparison between competing and non competing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the percentage of observed event at different time horizon\n",
    "for time in times:\n",
    "    print('At time {:.2f}'.format(time))\n",
    "    for risk in np.unique(e):\n",
    "        print('\\t {:.2f} % observed risk {}'.format(100 * ((e == risk) & (t < time)).mean(), risk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for risk in np.unique(e):\n",
    "    print('\\t {:.2f} % observed risk {}'.format(100 * ((e[e!=0] == risk)).mean(), risk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data into train, test and validation sets\n",
    "\n",
    "We will train NSC on 80% of the Data (10 % of which is used for stopping criterion and 10% for model Selection) and report performance on the remaining 20% held out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "x_train, x_test, t_train, t_test, e_train, e_test = train_test_split(x, t, e, test_size = 0.2, random_state = 42)\n",
    "x_train, x_val, t_train, t_val, e_train, e_val = train_test_split(x_train, t_train, e_train, test_size = 0.2, random_state = 42)\n",
    "x_dev, x_val, t_dev, t_val, e_dev, e_val = train_test_split(x_val, t_val, e_val, test_size = 0.5, random_state = 42)\n",
    "\n",
    "ss = MinMaxScaler().fit(t_train.reshape(-1, 1))\n",
    "t_train_ddh = ss.transform(t_train.reshape(-1, 1)).flatten()\n",
    "t_dev_ddh = ss.transform(t_dev.reshape(-1, 1)).flatten()\n",
    "t_val_ddh = ss.transform(t_val.reshape(-1, 1)).flatten()\n",
    "times_ddh = ss.transform(np.array(times).reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the parameter grid\n",
    "\n",
    "Lets set up the parameter grid to tune hyper-parameters. We will tune the number of underlying survival distributions, \n",
    "($K$), the distribution choices (Log-Normal or Weibull), the learning rate for the Adam optimizer between $1\\times10^{-3}$ and $1\\times10^{-4}$ and the number of hidden layers between $0, 1$ and $2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [[50, 50, 50]]\n",
    "param = {\n",
    "            'learning_rate' : [1e-3],\n",
    "            'layers_surv': layers,\n",
    "            'layers' : layers,\n",
    "            'batch': [100],\n",
    "            }\n",
    "n_iter = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nfg import NeuralFineGray\n",
    "from desurv import DeSurv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "desurv_times, fine_times = [], []\n",
    "# Ensures that we use the same architecture\n",
    "for _ in range(100):\n",
    "    desurv_time = time.process_time()\n",
    "    model = DeSurv(layers = param['layers'], layers_surv = param['layers_surv'], dropout = 0.25) \n",
    "    model.fit(x_train, t_train_ddh, e_train, n_iter = n_iter, bs = param['batch'], patience_max = n_iter, # Ensures that we train for n_iter iterations\n",
    "            lr = param['learning_rate'], val_data = (x_dev, t_dev_ddh, e_dev))\n",
    "    nll = model.compute_nll(x_val, t_val_ddh, e_val)\n",
    "    desurv_times.append(((time.process_time() - desurv_time) / n_iter, nll))\n",
    "\n",
    "    fine_time = time.process_time()\n",
    "    model = NeuralFineGray(layers = param['layers'], layers_surv = param['layers_surv'], dropout = 0.25)\n",
    "    model.fit(x_train, t_train_ddh, e_train, n_iter = n_iter, bs = param['batch'], patience_max = n_iter,\n",
    "            lr = param['learning_rate'], val_data = (x_dev, t_dev_ddh, e_dev))\n",
    "    nll = model.compute_nll(x_val, t_val_ddh, e_val)\n",
    "    fine_times.append(((time.process_time() - fine_time) / n_iter, nll))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display performance and time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desurv_times = pd.DataFrame(desurv_times, columns = ['Second / Epoch', 'NLL'])\n",
    "fine_times = pd.DataFrame(fine_times, columns = ['Second / Epoch', 'NLL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = desurv_times.plot.scatter('Second / Epoch', 'NLL', c = 'blue', alpha = 0.5, label = 'DeSurv')\n",
    "desurv_times.mean().to_frame().T.plot.scatter('Second / Epoch', 'NLL', c = 'blue', s = 50, marker = 'x', ax = ax)\n",
    "\n",
    "fine_times.plot.scatter('Second / Epoch', 'NLL', ax = ax, color = 'red', alpha = 0.5, label = 'Fine Gray')\n",
    "fine_times.mean().to_frame().T.plot.scatter('Second / Epoch', 'NLL', c = 'red', s = 50, marker = 'x', ax = ax)\n",
    "\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('survival')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:41:22) [Clang 13.0.1 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "f1b50223f39b64c0c24545f474e3e7d2d3b4b121fe045100fc03a3926bb649af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

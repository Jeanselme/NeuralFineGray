{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Fine Gray on FRAMINGHAM Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will apply Neural Fine Gray on the FRAMINGHAM data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../DeepSurvivalMachines/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the FRAMINGHAM Dataset\n",
    "\n",
    "The package includes helper functions to load the dataset.\n",
    "\n",
    "X represents an np.array of features (covariates),\n",
    "T is the event/censoring times and,\n",
    "E is the censoring indicator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nfg import datasets\n",
    "x, t, e, columns = datasets.load_dataset('FRAMINGHAM', path = '../', competing = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute horizons at which we evaluate the performance of Neural Fine Gray\n",
    "\n",
    "Survival predictions are issued at certain time horizons. Here we will evaluate the performance\n",
    "of NFG to issue predictions at the 25th, 50th and 75th event time quantile as is standard practice in Survival Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "np.random.seed(42)\n",
    "torch.random.manual_seed(42)\n",
    "\n",
    "horizons = [0.25, 0.5, 0.75]\n",
    "times = np.quantile(t[e > 0], horizons) # Fixed horizons for accurate comparison between competing and non competing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At time 0.01\n",
      "\t 0.03 % observed risk 0\n",
      "\t 15.40 % observed risk 1\n",
      "\t 6.49 % observed risk 2\n",
      "At time 0.05\n",
      "\t 0.24 % observed risk 0\n",
      "\t 27.08 % observed risk 1\n",
      "\t 16.70 % observed risk 2\n",
      "At time 0.24\n",
      "\t 1.12 % observed risk 0\n",
      "\t 38.09 % observed risk 1\n",
      "\t 27.58 % observed risk 2\n",
      "Total\n",
      "\t 12.44 % observed risk 0\n",
      "\t 48.31 % observed risk 1\n",
      "\t 39.25 % observed risk 2\n"
     ]
    }
   ],
   "source": [
    "# Display the percentage of observed event at different time horizon\n",
    "for time in times:\n",
    "    print('At time {:.2f}'.format(time))\n",
    "    for risk in np.unique(e):\n",
    "        print('\\t {:.2f} % observed risk {}'.format(100 * ((e == risk) & (t < time)).mean(), risk))\n",
    "\n",
    "print('Total')\n",
    "for risk in np.unique(e):\n",
    "    print('\\t {:.2f} % observed risk {}'.format(100 * ((e == risk)).mean(), risk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data into train, test and validation sets\n",
    "\n",
    "We will train NSC on 80% of the Data (10 % of which is used for stopping criterion and 10% for model Selection) and report performance on the remaining 20% held out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "x_train, x_test, t_train, t_test, e_train, e_test = train_test_split(x, t, e, test_size = 0.2, random_state = 42)\n",
    "x_train, x_val, t_train, t_val, e_train, e_val = train_test_split(x_train, t_train, e_train, test_size = 0.2, random_state = 42)\n",
    "x_dev, x_val, t_dev, t_val, e_dev, e_val = train_test_split(x_val, t_val, e_val, test_size = 0.5, random_state = 42)\n",
    "\n",
    "minmax = lambda x: x / t_train.max() # Enforce to be inferior to 1\n",
    "t_train_ddh = minmax(t_train)\n",
    "t_dev_ddh = minmax(t_dev)\n",
    "t_val_ddh = minmax(t_val)\n",
    "times_ddh = minmax(np.array(times))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the parameter grid\n",
    "\n",
    "Lets set up the parameter grid to tune hyper-parameters. We will tune the number of underlying survival distributions, \n",
    "($K$), the distribution choices (Log-Normal or Weibull), the learning rate for the Adam optimizer between $1\\times10^{-3}$ and $1\\times10^{-4}$ and the number of hidden layers between $0, 1$ and $2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [[50], [50, 50], [50, 50, 50], [100], [100, 100], [100, 100, 100]]\n",
    "param_grid = {\n",
    "            'learning_rate' : [1e-3, 1e-4],\n",
    "            'layers_surv': layers,\n",
    "            'layers' : layers,\n",
    "            'batch': [100, 250],\n",
    "            }\n",
    "params = ParameterSampler(param_grid, 1, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nfg import NeuralFineGray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -2.113:   6%|â–Œ         | 61/1000 [00:10<02:46,  5.65it/s]\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "for param in params:\n",
    "    model = NeuralFineGray(layers = param['layers'], layers_surv = param['layers_surv'])\n",
    "    # The fit method is called to train the model\n",
    "    model.fit(x_train, t_train_ddh, e_train, n_iter = 1000, bs = param['batch'],\n",
    "            lr = param['learning_rate'], val_data = (x_dev, t_dev_ddh, e_dev))\n",
    "    nll = model.compute_nll(x_val, t_val_ddh, e_val)\n",
    "    if not(np.isnan(nll)):\n",
    "        models.append([nll, model])\n",
    "    else:\n",
    "        print(\"WARNING: Nan Value Observed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = min(models, key = lambda x: x[0])\n",
    "model = best_model[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "Model prediction for the different patients and analysis of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_survival = model.predict_survival(x_test, minmax(np.linspace(0, 2, 1000)).tolist(), risk = risk)\n",
    "out_risk = 1 - out_survival"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "We evaluate the performance of NSC in its discriminative ability (Time Dependent Concordance Index and Cumulative Dynamic AUC) as well as Brier Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sksurv.metrics import concordance_index_ipcw, brier_score, cumulative_dynamic_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "time must be smaller than largest observed time point: 3.2490276362749237",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/vincent/Desktop/Turing/Projects/NeuralFineGray/examples/Neural Fine Gray on FRAMINGHAM Dataset.ipynb Cell 24\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/vincent/Desktop/Turing/Projects/NeuralFineGray/examples/Neural%20Fine%20Gray%20on%20FRAMINGHAM%20Dataset.ipynb#X32sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, _ \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(times):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/vincent/Desktop/Turing/Projects/NeuralFineGray/examples/Neural%20Fine%20Gray%20on%20FRAMINGHAM%20Dataset.ipynb#X32sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     cis\u001b[39m.\u001b[39mappend(concordance_index_ipcw(et_train, et_test[selection], out_risk[:, i][selection], times[i])[\u001b[39m0\u001b[39m])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/vincent/Desktop/Turing/Projects/NeuralFineGray/examples/Neural%20Fine%20Gray%20on%20FRAMINGHAM%20Dataset.ipynb#X32sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m brs \u001b[39m=\u001b[39m brier_score(et_train, et_test[selection], out_survival[selection], times)[\u001b[39m1\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vincent/Desktop/Turing/Projects/NeuralFineGray/examples/Neural%20Fine%20Gray%20on%20FRAMINGHAM%20Dataset.ipynb#X32sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m roc_auc \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vincent/Desktop/Turing/Projects/NeuralFineGray/examples/Neural%20Fine%20Gray%20on%20FRAMINGHAM%20Dataset.ipynb#X32sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, _ \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(times):\n",
      "File \u001b[0;32m~/miniconda3/envs/Jupyter/lib/python3.9/site-packages/sksurv/metrics.py:625\u001b[0m, in \u001b[0;36mbrier_score\u001b[0;34m(survival_train, survival_test, estimate, times)\u001b[0m\n\u001b[1;32m    623\u001b[0m prob_cens_t[prob_cens_t \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39minf\n\u001b[1;32m    624\u001b[0m \u001b[39m# calculate inverse probability of censoring weights at observed time point\u001b[39;00m\n\u001b[0;32m--> 625\u001b[0m prob_cens_y \u001b[39m=\u001b[39m cens\u001b[39m.\u001b[39;49mpredict_proba(test_time)\n\u001b[1;32m    626\u001b[0m prob_cens_y[prob_cens_y \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39minf\n\u001b[1;32m    628\u001b[0m \u001b[39m# Calculating the brier scores at each time point\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/Jupyter/lib/python3.9/site-packages/sksurv/nonparametric.py:380\u001b[0m, in \u001b[0;36mSurvivalFunctionEstimator.predict_proba\u001b[0;34m(self, time)\u001b[0m\n\u001b[1;32m    378\u001b[0m extends \u001b[39m=\u001b[39m time \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munique_time_[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    379\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprob_[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m extends\u001b[39m.\u001b[39many():\n\u001b[0;32m--> 380\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mtime must be smaller than largest \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    381\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39mobserved time point: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39munique_time_[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]))\n\u001b[1;32m    383\u001b[0m \u001b[39m# beyond last time point is zero probability\u001b[39;00m\n\u001b[1;32m    384\u001b[0m Shat \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39mempty(time\u001b[39m.\u001b[39mshape, dtype\u001b[39m=\u001b[39m\u001b[39mfloat\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: time must be smaller than largest observed time point: 3.2490276362749237"
     ]
    }
   ],
   "source": [
    "et_train = np.array([(e_train[i] == risk, t_train[i]) for i in range(len(e_train))],\n",
    "                 dtype = [('e', bool), ('t', float)])\n",
    "et_test = np.array([(e_test[i] == risk, t_test[i]) for i in range(len(e_test))],\n",
    "                 dtype = [('e', bool), ('t', float)])\n",
    "selection = (t_test < t_train.max()) | (e_test != risk)\n",
    "\n",
    "cis = []\n",
    "for i, _ in enumerate(times):\n",
    "    cis.append(concordance_index_ipcw(et_train, et_test[selection], out_risk[:, i][selection], times[i])[0])\n",
    "brs = brier_score(et_train, et_test[selection], out_survival[selection], times)[1]\n",
    "roc_auc = []\n",
    "for i, _ in enumerate(times):\n",
    "    roc_auc.append(cumulative_dynamic_auc(et_train, et_test[selection], out_risk[:, i][selection], times[i])[0])\n",
    "for horizon in enumerate(horizons):\n",
    "    print(f\"For {horizon[1]} quantile,\")\n",
    "    print(\"TD Concordance Index:\", cis[horizon[0]])\n",
    "    print(\"Brier Score:\", brs[horizon[0]])\n",
    "    print(\"ROC AUC \", roc_auc[horizon[0]][0], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('survival')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "vscode": {
   "interpreter": {
    "hash": "f1b50223f39b64c0c24545f474e3e7d2d3b4b121fe045100fc03a3926bb649af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

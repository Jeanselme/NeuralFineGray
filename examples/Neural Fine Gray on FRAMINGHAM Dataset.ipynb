{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Fine Gray on FRAMINGHAM Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will apply Neural Fine Gray on the FRAMINGHAM data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../DeepSurvivalMachines/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the FRAMINGHAM Dataset\n",
    "\n",
    "The package includes helper functions to load the dataset.\n",
    "\n",
    "X represents an np.array of features (covariates),\n",
    "T is the event/censoring times and,\n",
    "E is the censoring indicator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nfg import datasets\n",
    "x, t, e, columns = datasets.load_dataset('FRAMINGHAM', path = '../', competing = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute horizons at which we evaluate the performance of Neural Fine Gray\n",
    "\n",
    "Survival predictions are issued at certain time horizons. Here we will evaluate the performance\n",
    "of NFG to issue predictions at the 25th, 50th and 75th event time quantile as is standard practice in Survival Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "np.random.seed(42)\n",
    "torch.random.manual_seed(42)\n",
    "\n",
    "horizons = [0.25, 0.5, 0.75]\n",
    "times = np.quantile(t[e > 0], horizons) # Fixed horizons for accurate comparison between competing and non competing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the percentage of observed event at different time horizon\n",
    "for time in times:\n",
    "    print('At time {:.2f}'.format(time))\n",
    "    for risk in np.unique(e):\n",
    "        print('\\t {:.2f} % observed risk {}'.format(100 * ((e == risk) & (t < time)).mean(), risk))\n",
    "\n",
    "print('Total')\n",
    "for risk in np.unique(e):\n",
    "    print('\\t {:.2f} % observed risk {}'.format(100 * ((e == risk)).mean(), risk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data into train, test and validation sets\n",
    "\n",
    "We will train NFG on 80% of the Data (10 % of which is used for stopping criterion and 10% for model Selection) and report performance on the remaining 20% held out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, t_train, t_test, e_train, e_test = train_test_split(x, t, e, test_size = 0.2, random_state = 42)\n",
    "x_train, x_val, t_train, t_val, e_train, e_val = train_test_split(x_train, t_train, e_train, test_size = 0.2, random_state = 42)\n",
    "x_dev, x_val, t_dev, t_val, e_dev, e_val = train_test_split(x_val, t_val, e_val, test_size = 0.5, random_state = 42)\n",
    "\n",
    "# Time normalisaiton is critical as it is an input of the network\n",
    "# Time 0 must also be 0 so we use min max\n",
    "minmax = lambda x: x / t_train.max() # Enforce to be inferior to 1\n",
    "t_train_ddh = minmax(t_train)\n",
    "t_dev_ddh = minmax(t_dev)\n",
    "t_val_ddh = minmax(t_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the parameter grid\n",
    "\n",
    "As a constrained neural network, training is sensitive to the hyperparameters, we recommend a grid search as followed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [[50], [50, 50], [50, 50, 50], [100], [100, 100], [100, 100, 100]]\n",
    "param_grid = {\n",
    "            'learning_rate' : [1e-3, 1e-4],\n",
    "            'layers_surv': layers,\n",
    "            'layers' : layers,\n",
    "            'batch': [100, 250],\n",
    "            }\n",
    "params = ParameterSampler(param_grid, 5, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nfg import NeuralFineGray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "for param in params:\n",
    "    model = NeuralFineGray(layers = param['layers'], layers_surv = param['layers_surv'])\n",
    "    # The fit method is called to train the model\n",
    "    model.fit(x_train, t_train_ddh, e_train, n_iter = 1000, bs = param['batch'],\n",
    "            lr = param['learning_rate'], val_data = (x_dev, t_dev_ddh, e_dev))\n",
    "    nll = model.compute_nll(x_val, t_val_ddh, e_val)\n",
    "    if not(np.isnan(nll)):\n",
    "        models.append([nll, model])\n",
    "    else:\n",
    "        print(\"WARNING: Nan Value Observed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = min(models, key = lambda x: x[0])\n",
    "model = best_model[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "Model prediction for the different patients and analysis of the results. As we use cumulative metrics, we predict over a grid of points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_times = np.linspace(0, t_train.max(), 100)\n",
    "out_survival = model.predict_survival(x_test, minmax(pred_times).tolist())\n",
    "out_risk = 1 - out_survival"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "We evaluate the performance of NFG in its discriminative ability (Time Dependent Concordance Index and Cumulative Dynamic AUC) as well as Brier Score. Note that we implemented competing risks metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import truncated_concordance_td, auc_td, brier_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cis, brs, rocs = [], [], []\n",
    "km = (e_train, t_train)\n",
    "for i, te in enumerate(times):\n",
    "    # Compute metrics for risk = 1 and estimate the km used for IPCW\n",
    "    ci, km = truncated_concordance_td(e_test, t_test, out_risk, pred_times, te, km = km, risk = 1) \n",
    "    cis.append(ci)\n",
    "    brs.append(brier_score(e_test, t_test, out_risk, pred_times, te, km = km, risk = 1)[0])\n",
    "    rocs.append(auc_td(e_test, t_test, out_risk, pred_times, te, km = km, risk = 1)[0])\n",
    "\n",
    "for i, horizon in enumerate(horizons):\n",
    "    print(f\"For {horizon} quantile,\")\n",
    "    print(\"Truncated Concordance Index: {:.3f}\".format(cis[i]))\n",
    "    print(\"tdAUC: {:.3f}\".format(rocs[i]))\n",
    "    print(\"Brier Score: {:.3f}\".format(brs[i]))\n",
    "    print('*' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('survival')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "vscode": {
   "interpreter": {
    "hash": "f1b50223f39b64c0c24545f474e3e7d2d3b4b121fe045100fc03a3926bb649af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
